Metadata-Version: 2.4
Name: doro
Version: 0.1.0
Summary: Q-Learning Agent for Anomaly Detection and Investigation in Robotics
Keywords: machine-learning,robotics,q-learning,anomaly-detection,computer-vision
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Requires-Python: >=3.11
Description-Content-Type: text/markdown
Requires-Dist: torch>=1.9.0
Requires-Dist: torchvision>=0.10.0
Requires-Dist: opencv-python>=4.5.0
Requires-Dist: numpy>=1.21.0
Requires-Dist: matplotlib>=3.4.0
Requires-Dist: websockets>=10.0
Provides-Extra: dev
Requires-Dist: pytest>=6.0; extra == "dev"

# DORO: Deep Q-Learning Robotic Operator for Anomaly Detection

DORO (Deep Q-Learning Robotic Operator) is a distributed reinforcement learning system that enables autonomous robots to detect and investigate anomalies using computer vision and deep Q-learning. The system features a client-server architecture where robots collect sensor data and powerful computers handle the AI processing.

## ğŸ—ï¸ Project Architecture

```mermaid
graph TB
    subgraph "DORO System Architecture"
        subgraph "Robot Hardware"
            RPI[Raspberry Pi 3]
            CAM[Camera Module]
            USS[Ultrasonic Sensor]
            MOT[Motors & Actuators]
        end
        
        subgraph "Server Component"
            RS[Robot Server<br/>src/doro/server/]
            RS --> RPI
            RPI --> CAM
            RPI --> USS
            RPI --> MOT
        end
        
        subgraph "Client Component"
            RC[Robot Client<br/>src/doro/client/]
            AG[Q-Learning Agent<br/>src/doro/agent/]
            ENV[Environment Interface<br/>src/doro/environment/]
        end
        
        subgraph "AI Models"
            DQN[Deep Q-Network]
            LSTM[LSTM Feature Extractor]
            RN50[ResNet50 Anomaly Detector]
        end
        
        subgraph "Training & Utils"
            TRN[Training Pipeline<br/>src/doro/utils/]
            DEMO[Demo System<br/>demo.py]
        end
        
        RS <-->|TCP Socket<br/>Sensor Data| RC
        RC --> AG
        AG --> ENV
        AG --> DQN
        AG --> LSTM
        AG --> RN50
        ENV --> RC
        TRN --> AG
        DEMO --> AG
    end
```

## ğŸ“ Project Structure

```
tfg-fer/
â”œâ”€â”€ src/doro/                    # Main DORO package
â”‚   â”œâ”€â”€ agent/                   # Q-Learning agent implementation
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ q_learning_agent.py  # Deep Q-Network and anomaly detection
â”‚   â”œâ”€â”€ client/                  # Client-side robot communication
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ robot_client.py      # Network client for robot control
â”‚   â”œâ”€â”€ server/                  # Server-side robot hardware interface
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ robot_server.py      # Robot hardware server
â”‚   â”œâ”€â”€ environment/             # Environment abstractions
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py             # Base environment interface
â”‚   â”‚   â”œâ”€â”€ simulated.py        # Simulated environment for training
â”‚   â”‚   â””â”€â”€ real.py             # Real robot environment
â”‚   â”œâ”€â”€ utils/                   # Training and utility functions
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ train_agent.py      # Training pipeline and utilities
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ docs/                        # Documentation
â”‚   â”œâ”€â”€ CLIENT_SERVER_SETUP.md   # Distributed setup guide
â”‚   â””â”€â”€ ULTRASONIC_INTEGRATION.md # Hardware integration guide
â”œâ”€â”€ tests/                       # Unit tests
â”œâ”€â”€ scripts/                     # Deployment and setup scripts
â”œâ”€â”€ demo.py                      # Interactive demo system
â”œâ”€â”€ setup_raspberry_pi.sh        # Raspberry Pi setup script
â”œâ”€â”€ pyproject.toml              # Project configuration
â”œâ”€â”€ uv.lock                     # Dependency lock file
â””â”€â”€ README.md                   # This file
```

## âœ¨ Features

- **Distributed Architecture**: Client-server design separating AI processing from robot hardware
- **Deep Q-Network (DQN)** with experience replay and target networks
- **LSTM-based temporal feature extraction** for processing sequences of 5 frames
- **Anomaly detection** using embedding comparison between LSTM and ResNet50 models
- **6-action robot control**: forward, backward, left, right, turn left, turn right
- **Real-time sensor integration**: Camera and ultrasonic distance sensing
- **Comprehensive training framework** with progress logging and visualization
- **Interactive demo system** for manual control and testing
- **Modular environment system** supporting both simulated and real robot environments

## ğŸš€ Quick Start

### 1. Installation

```bash
git clone <repository-url>
cd tfg-fer
uv sync
```

### 2. Training (Simulated Environment)

```bash
# Basic training with default parameters
python -m doro.utils.train_agent

# Advanced training with custom parameters
python -m doro.utils.train_agent --episodes 2000 --render --device cuda
```

### 3. Real Robot Setup

#### Raspberry Pi (Robot Server)
```bash
# Automated setup
./setup_raspberry_pi.sh
# Manual start
python -m doro.server.robot_server
```

#### Computer (Q-Learning Client)
```bash
# Connect to robot at IP 192.168.1.100
python -m doro.client.robot_client --robot-host 192.168.1.100
```

### 4. Interactive Demo

```bash
# Run automated demo
python demo.py --model models/final/ --episodes 10

# Interactive control mode
python demo.py --interactive --model models/final/
```

## ğŸ§  Architecture Details

### Deep Q-Network Architecture
```
Input (2049) â†’ FC(512) â†’ Dropout â†’ FC(512) â†’ Dropout â†’ FC(256) â†’ FC(6)
     ^
     LSTM(2048) + Ultrasonic(1)
```

### LSTM Feature Extractor
```
Input (5Ã—2048) â†’ LSTM(512Ã—2) â†’ FC(2048)
```

### Anomaly Detection Pipeline
1. **Frame Capture**: 640Ã—480 RGB frames from robot camera
2. **Feature Extraction**: LSTM processes sequence of 5 frames â†’ 2048D embedding
3. **Baseline Comparison**: ResNet50 generates reference embeddings
4. **Anomaly Score**: Cosine similarity between LSTM and ResNet50 embeddings
5. **Threshold Decision**: Anomaly detected if similarity < 0.5

### Reward System

The agent receives structured rewards to encourage safe anomaly investigation:

- **ğŸ›¡ï¸ Collision Avoidance**: -2.0 for risky forward movement, +0.5 for safe backing
- **ğŸ” Anomaly Investigation**: +2.0 for approaching detected anomalies safely  
- **ğŸ§  Smart Navigation**: +1.0 for strategic side movements around obstacles
- **ğŸ“ Distance Awareness**: +0.05 bonus for maintaining >50cm clearance
- **âš¡ Exploration**: +0.1 for forward/lateral movements
- **â±ï¸ Efficiency**: -0.01 per step to encourage task completion

## âš™ï¸ Configuration

### Hyperparameters
```python
learning_rate = 1e-4      # Adam optimizer learning rate
gamma = 0.99              # Reward discount factor
epsilon = 1.0             # Initial exploration probability
epsilon_min = 0.01        # Minimum exploration probability
epsilon_decay = 0.995     # Exploration decay rate
memory_size = 10000       # Experience replay buffer size
batch_size = 32           # Training batch size
target_update_freq = 100  # Target network update frequency
```

### Environment Parameters
```python
width = 640               # Camera frame width
height = 480              # Camera frame height
max_steps = 500           # Maximum steps per episode
anomaly_probability = 0.1 # Probability of anomalies in simulation
ultrasonic_threshold = 30 # Collision avoidance distance (cm)
```

## ğŸ”§ Hardware Integration

### Supported Hardware
- **Single Board Computer**: Raspberry Pi 4B/3B+
- **Camera**: RPi Camera Module or USB camera
- **Distance Sensor**: HC-SR04 ultrasonic sensor
- **Motors**: Compatible with L298N motor driver
- **Power**: 5V/3A power supply recommended

### GPIO Configuration
```python
# Raspberry Pi GPIO pins
ULTRASONIC_TRIGGER = 18   # GPIO 18 (Pin 12)
ULTRASONIC_ECHO = 24      # GPIO 24 (Pin 18)
MOTOR_LEFT_FWD = 2        # GPIO 2 (Pin 3)
MOTOR_LEFT_BWD = 3        # GPIO 3 (Pin 5)
MOTOR_RIGHT_FWD = 4       # GPIO 4 (Pin 7)
MOTOR_RIGHT_BWD = 14      # GPIO 14 (Pin 8)
```

## ğŸ“Š Performance Metrics

The training system tracks comprehensive metrics:

- **Episode Rewards**: Cumulative reward per episode
- **Anomaly Detection Rate**: Percentage of anomalies successfully detected
- **Collision Rate**: Safety metric tracking obstacle avoidance
- **Exploration Efficiency**: Coverage of environment space
- **Learning Convergence**: Q-value stabilization over time

## ğŸ”¬ Research Applications

DORO is designed for research in:

- **Autonomous Navigation**: Safe robot movement in unknown environments
- **Anomaly Detection**: Computer vision-based anomaly identification
- **Reinforcement Learning**: Deep Q-learning in real-world robotics
- **Distributed AI**: Client-server AI architectures for resource-constrained robots
- **Human-Robot Interaction**: Safe autonomous behavior in human environments

## ğŸ“– Documentation

Detailed documentation is available in the `docs/` directory:

- [Client-Server Setup Guide](docs/CLIENT_SERVER_SETUP.md) - Complete setup instructions for distributed deployment
- [Ultrasonic Integration Guide](docs/ULTRASONIC_INTEGRATION.md) - Hardware sensor integration details
